# Image & Video Captioning System (Open-Source, No Proprietary APIs)

This project implements an image and video captioning system using open-source models only.
It fulfills all requirements specified in the internship assignment, including advanced post-processing to ensure high-quality, non-repetitive video summaries.

## Features
- Image Captioning
  - Accepts any image (.jpg, .png, .jpeg)
  - Uses BLIP (Salesforce/blip-image-captioning-base)
  - Generates a short natural-language caption
- Video Captioning (Advanced Summary)
  - Extracts **10 evenly-spaced frames** using OpenCV.
  - **Filters for unique captions** across these frames to identify scene changes.
  - Uses **DistilBART** for concise text summarization.
  - Generates a final, non-repetitive video summary.
  - No scene detection required.
- No Proprietary APIs
  - No GPT, Claude, Gemini, or paid services
  - Entirely open-source + local PyTorch inference

## Project Structure
image-video-captioning/
|
|-- caption_image.py
|-- caption_video.py
|-- requirements.txt
|-- README.md
|
|-- samples/        # Add your test images and videos here
|-- outputs/        # Captions generated by the scripts
|-- blip/           # Local folder for BLIP model files
|-- distilbart-summarizer/ # Local folder for DistilBART model files

## Setup Instructions
1. Create Virtual Environment
python -m venv venv

Activate:

Windows
venv\Scripts\activate

Mac/Linux
source venv/bin/activate

2. Install Dependencies
pip install -r requirements.txt

3. Download Models Locally
Since this project must run entirely offline, you must manually download the model files and place them in the correct directory structure:

#### A. BLIP Model (Image Captioning)
Download the necessary files for `Salesforce/blip-image-captioning-base` and place them inside the **`./blip`** directory.

**Required Files:**
- `config.json`
- `pytorch_model.bin` (The main model weights)
- `vocab.txt`
- `special_tokens_map.json`
- `tokenizer_config.json`

#### B. DistilBART Summarizer (Video Summary Refinement)
Download the necessary files for `sshleifer/distilbart-cnn-12-6` and place them inside the **`./distilbart-summarizer`** directory.

**Required Files:**
- `config.json`
- `pytorch_model.bin` (The main model weights)
- `tokenizer_config.json`
- `vocab.json`
- `merges.txt`

## How to Run Image Captioning
python caption_image.py samples/sample.jpg

Example output:

"A brown dog running in a grassy field."

## How to Run Video Captioning
python caption_video.py samples/dynamic_video.mp4

Example output (Note the concise, combined summary):

UNIQUE FRAME CAPTIONS:
- Frame 1 @ 130.03s: a man in a suit and tie standing in front of a dark background
- Frame 2 @ 260.09s: a woman is standing in front of a window
- Frame 7 @ 910.38s: a man is playing a game with another man

FINAL VIDEO CAPTION (Summarized):
A man in a suit and tie is seen standing in a dark background, followed by a woman in front of a window, and a final scene of a man playing a game.

## Models Used

| Model | Purpose | Reason for Choosing |
| :--- | :--- | :--- |
| **BLIP** (Salesforce/blip-image-captioning-base) | Image Captioning | Lightweight, fast inference, high-quality general captions, completely open-source. |
| **DistilBART-CNN-12-6** (sshleifer/distilbart-cnn-12-6) | Text Summarization | Used as a **post-processing step** to take the stream of unique BLIP captions and generate a single, non-repetitive, concise summary sentence, resolving redundancy issues inherent in vision models. |

## Assignment Requirements Coverage
| Requirement | Status |
| :--- | :--- |
| Image captioning | ✔ Completed |
| Video captioning | ✔ Completed |
| Open-source only | ✔ No proprietary APIs used |
| CLI scripts | ✔ Provided |
| Sample outputs | ✔ Included |
| README | ✔ Complete and detailed |
| Git-ready structure | ✔ Fully structured |

## License

MIT License — free to use, modify, share.